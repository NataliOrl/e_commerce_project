# -*- coding: utf-8 -*-
"""Копия блокнота "e-commerce project КарповКурс.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ArXZQ4S3YQpwQL2xCYilmadWc7l1g2yg

# Описание проекта
"""

# Commented out IPython magic to ensure Python compatibility.
#!pip install pingouin
import pandas as pd
import statistics
import numpy as np
import scipy.stats
#import pingouin as pg


# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

import plotly.express as px
plt.style.use('dark_background')

sns.set(rc={'figure.figsize':(16,6)}, style='darkgrid')
pd.options.mode.chained_assignment = None

"""Для решения задачи проведи предварительное исследование данных и сформулируй, что должно считаться покупкой. Обосновать свой выбор ты можешь с помощью фактов оплат, статусов заказов и других имеющихся данных.

Файлы:

## Customers (olist_customers)
"""

! gdown --id 1e7UFs0aDVxiCoKaaGw5QNm477cZKLRJd

"""

 olist_customers_datase.csv — таблица с уникальными идентификаторами пользователей https://drive.google.com/file/d/1e7UFs0aDVxiCoKaaGw5QNm477cZKLRJd/view?usp=sharing

- customer_id — позаказный идентификатор пользователя

- customer_unique_id —  уникальный идентификатор пользователя  (аналог номера паспорта)

- customer_zip_code_prefix —  почтовый индекс пользователя

- customer_city —  город доставки пользователя

- customer_state —  штат доставки пользователя"""

customers = pd.read_csv('/content/olist_customers_dataset.csv')
customers.head()

"""## Orders (olist_orders)"""

! gdown --id 1SCMMwxJRgEXC5lVjo1cg-NU5av-s1z6H

"""olist_orders_dataset.csv —  таблица заказов

https://drive.google.com/file/d/1SCMMwxJRgEXC5lVjo1cg-NU5av-s1z6H/view?usp=sharing

- order_id —  уникальный идентификатор заказа (номер чека)

- customer_id —  позаказный идентификатор пользователя

- order_status —  статус заказа

- order_purchase_timestamp —  время создания заказа

- order_approved_at —  время подтверждения оплаты заказа

- order_delivered_carrier_date —  время передачи заказа в логистическую службу

- order_delivered_customer_date —  время доставки заказа

- order_estimated_delivery_date —  обещанная дата доставки

Уникальные статусы заказов в таблице olist_orders_dataset:

- created —  создан
- approved —  подтверждён
- invoiced —  выставлен счёт
- processing —  в процессе сборки заказа
- shipped —  отгружен со склада
- delivered —  доставлен пользователю
- unavailable —  недоступен
- canceled —  отменён
"""

orders = pd.read_csv('/content/olist_orders_dataset.csv')
orders.shape

orders['order_purchase_timestamp']=pd.to_datetime(orders['order_purchase_timestamp'])

"""## Items (olist_order_items)"""

!gdown --id 1PQWLSgik_ma43i42rRICKI2GxbHr0CI8

"""olist_order_items_dataset.csv —  товарные позиции, входящие в заказы
https://drive.google.com/file/d/1PQWLSgik_ma43i42rRICKI2GxbHr0CI8/view?usp=sharing

- order_id —  уникальный идентификатор заказа (номер чека)

- order_item_id —  идентификатор товара внутри одного заказа

- product_id —  ид товара (аналог штрихкода)

- seller_id — ид производителя товара

- shipping_limit_date —  максимальная дата доставки продавцом для передачи заказа партнеру по логистике

- price —  цена за единицу товара

- freight_value —  вес товар-а

- — Пример структуры данных можно визуализировать по order_id == 00143d0f86d6fbd9f9b38ab440ac16f5
"""

items =pd.read_csv('/content/olist_order_items_dataset.csv')
items.head()

"""## 1- Cколько пользователей совершили покупку 1 раз (90536)

Сколько у нас пользователей, которые совершили покупку только один раз? (7 баллов)

- Т.к. нет одельного статуса на совершение покупки, будем считать, что статус Заказа - delivered — доставлен пользователю и бы оплачен, равнозначен совершением покупки пользователе.

- Посчитаем количество соверешенных заказов (количество customer_id  у каждого customer_unique_id )
"""

#customers['customer_unique_id'].value_counts()
# покупателей сделавших заказы - 96 096

#customers['customer_id'].value_counts()
# всего сделано заказов - 99 441

orders.head()

df_delivd = orders[orders['order_status'] =="delivered"]
# отбираем заказы исполненные = совершению покупки = 96 478
df_delivd.shape

# объединяем таблицы  с покупателями и с заказами со статусом delivered
custom_delivd = df_delivd.merge(customers,how='left',on='customer_id')
custom_delivd.shape

#custom_delivd.head(3)

#custom_delivd_del = custom_delivd.dropna()
#удаляем пропуски
#custom_delivd_del.shape

custom_delivd.groupby('customer_unique_id').agg({'customer_id' :'count'}).value_counts()

"""## 2- Сколько заказов в месяц в среднем не доставляется по разным причинам (вывести детализацию по причинам)? (10 баллов)"""

orders.order_estimated_delivery_date = pd.to_datetime(orders.order_estimated_delivery_date)
orders['order_estimated_delivery_mance'] = orders.order_estimated_delivery_date.dt.strftime('%Y.%m')

orders.head(3)

orders.info()

orders.query('order_id == "00143d0f86d6fbd9f9b38ab440ac16f5"')

"""* Поскольку в запросе нет четких временных границ и указание какой именно временной промежуток следует изучить, предположим, что есть необходимость посмотреть вообщем сколько было в среднем недоставленных заказам в обещанные сроки за весь имеющийся период.

* Также исходя из стуктуры данных можно предположить, что при изменении статуса заказа в базе данных меняется именно статус "order_status" с указанием даты изменения, а не путем добавления новой строки с "order_id"


"""

no_deliv = orders.query('order_status != "delivered"')\
.groupby(['order_status','order_estimated_delivery_mance'], as_index = False).agg({'order_id':'count'})\
.rename(columns={'order_id':'mean_no_deliv'})\
.groupby('order_status', as_index = False).agg({'mean_no_deliv':'mean'})

no_deliv

#второй способ подсчета
cancel_orders = orders.query('order_status != "delivered"')\
    .groupby([pd.Grouper(freq='M', key='order_estimated_delivery_date'),'order_status'],as_index=False)\
    .agg({'order_id':'count'})\
    .groupby('order_status',as_index=False)\
    .agg({'order_id':'mean'})\
    .rename(columns={'order_id':'mean_no_delivered'})

cancel_orders['perc'] = (cancel_orders['mean_no_delivered']/cancel_orders['mean_no_delivered'].sum())*100
cancel_orders.round(2)

fig, scatter = plt.subplots(figsize = (9,6))

sns.barplot(x='order_status',y='mean_no_delivered', data=cancel_orders)

"""Уникальные статусы заказов в таблице olist_orders_dataset:

- created — создан
- approved — подтверждён
- invoiced — выставлен счёт
- processing — в процессе сборки заказа
- shipped — отгружен со склада
- delivered — доставлен пользователю
- unavailable — недоступен
- canceled — отменён

Посмотрим какое количество недоставленных товаров в назначенную дату "estimated_delivery_date" моесяцам за каждый год в имеющихся данных
"""

no_deliv_m = orders.query('order_status != "delivered"')\
.groupby('order_estimated_delivery_mance', as_index = False).agg({'order_id':'count'})\
.sort_values(['order_id'], ascending = False)

no_deliv_m['mean'] = no_deliv_m['order_id']/no_deliv_m['order_id'].sum()
no_deliv_m

"""самое большее количество недоставленных товаров в срок было в декабре 2017.
Посмотрим по каким основным причинам были причинам были не исполнены сроки
"""

no_deliv_ms = orders.loc[((orders['order_status'] != 'delivered') &(orders['order_estimated_delivery_mance'] == '2017.12'))]\
.groupby(['order_estimated_delivery_mance', 'order_status'], as_index = False).agg({'order_id':'count'})\
.sort_values(['order_id'], ascending = False)
no_deliv_ms

"""## 3- По каждому товару определить, в какой день недели товар чаще всего покупается. (7 баллов)"""

items.shape

orders.shape

# объединяем таблицы  с заказами  и товарами
items_orders = items.merge(orders,how='left',on='order_id')
items_orders.shape

items_orders.isna().mean().sort_values(ascending=False)

items_orders.fillna(0)

items_orders['purchase_day'] = pd.to_datetime(items_orders.order_purchase_timestamp)
items_orders['purchase_day'] = items_orders.purchase_day.dt.strftime('%A')
df=items_orders[['product_id', 'purchase_day','order_id']]
df=df.groupby(['product_id','purchase_day'], dropna=False, as_index=False)['order_id'].agg(['count']).sort_values('count')
df

df.query('product_id == "aca2eb7d00ea1a7b8ebd4e68314663af"')
#для проверки

purchase_day= df.loc[df.groupby(['product_id'])['count'].idxmax()].sort_values('count', ascending=False)
purchase_day

purchase_day.query('product_id == "aca2eb7d00ea1a7b8ebd4e68314663af"')
# для проверки

#df_days = df.pivot_table(index='product_id',columns='approved_day',values='order_id',aggfunc=['count'])
#df_days.max()

"""## 4- Сколько у каждого из пользователей в среднем покупок в неделю (по месяцам)

Не стоит забывать, что внутри месяца может быть не целое количество недель. Например, в ноябре 2021 года 4,28 недели. И внутри метрики это нужно учесть. (8 баллов)
"""

orders.order_purchase_timestamp = pd.to_datetime(orders.order_purchase_timestamp)

custom_delivd['month'] = orders.order_purchase_timestamp.dt.month_name()
# переводим дату в формат названия месяца
custom_delivd['week_values'] = orders.order_purchase_timestamp.dt.days_in_month / 7
# вычисяем кол-во недель в месяце
custom_delivd.head()

purchases_month  = custom_delivd.groupby(['customer_unique_id', 'month', 'week_values'], as_index=False).agg({'order_id':'count'})

purchases_month['avg_orders'] = (purchases_month.order_id/purchases_month.week_values)
purchases_month.sort_values('avg_orders', ascending = False)

"""##5- Когортный анализ

Используя pandas, проведи когортный анализ пользователей. В период с января по декабрь выяви когорту с самым высоким retention на 3й месяц.
"""

custom_delivd.info()

custom_delivd['purshase_period'] = custom_delivd.order_purchase_timestamp.dt.strftime('%Y-%m')
custom_delivd.head()

#отберем нужные данные для посторения когорты
df = custom_delivd[['customer_unique_id', 'order_id','purshase_period','order_purchase_timestamp']]
df.head()

"""Проверим, были ли уникальные пользовалети в последующие периоды.


"""

df.groupby(['purshase_period'])['customer_unique_id'].agg(['count','nunique']).sort_values('purshase_period')

"""Чтобы построить когорты, нам нужно сгруппировать клиентов по дате их первой покупки. В данных нет такого поля, значит, нужно его посчитать."""

df.set_index('customer_unique_id', inplace=True) #добавим индекс в dataFrame по customer_unique_id

#добавим столбец first_monht
df['first_monht'] = df.groupby(level=0)['order_purchase_timestamp'].min().dt.strftime('%Y-%m')

#df['first_monht'] = df.loc[:, ('order_purchase_timestamp')].min()
df.reset_index(inplace=True) #переиндексируем df

df.head()

cohorts = df.groupby(['first_monht','purshase_period'], as_index=False)\
.agg({'customer_unique_id': pd.Series.nunique})\
.rename(columns={'customer_unique_id': 'unique_users'})\
.sort_values(['first_monht','purshase_period'])

#df.groupby(['first_monht','purshase_period'])['customer_unique_id'].agg(['count','nunique']).sort_values(['first_monht','purshase_period'])

def cohort_period(df):
    df['cohort_period'] = np.arange(len(df)) + 1 # отсчет с 1
    return df
cohorts = cohorts.groupby('first_monht').apply(cohort_period)
cohorts

retention = cohorts.pivot_table(index='first_monht',columns='cohort_period',values='unique_users', aggfunc='mean')
retention.head(3)

x = retention[1]
x

retention_pr = retention.divide(x, axis=0).round(4)

retention[retention[3] == retention[3].max()].index

plt.rcParams['font.size'] = '12'
plt.figure(figsize=(18,14))
plt.title('Users Active')
ax = sns.heatmap(data=retention_pr, annot=True, vmin=0.0,vmax=0.008 ,cmap='crest')
ax.set_yticklabels(retention_pr.index)
fig=ax.get_figure()
#fig.savefig("Retention Counts.png")

plt.show()

import plotly.express as px
fig = px.imshow(retention_pr, text_auto=True, aspect="auto")
fig.show()

"""##6- RFM-сегментация пользователей"""

from datetime import date,timedelta
orders['order_purchase_timestamp']=pd.to_datetime(orders['order_purchase_timestamp'])
merged= pd.merge(customers, orders, on="customer_id")
merged= merged.merge(items, on="order_id")
merged.shape

merged= merged[merged['order_status'] =="delivered"]
merged_df = merged.dropna(axis=0)
len(merged_df[merged_df.duplicated()])
merged_df.info()

"""Так как датасет не самый свежий, вместо текущей даты будем использовать max+2. Для создания recency, frequency и monetary мы сгруппируем наши записи по customer_id."""

present_day = merged_df['order_purchase_timestamp'].max() + timedelta(days=2)
present_day

print("Latest date in dataset: ", merged_df['order_purchase_timestamp'].max())

rfm_df = merged_df.reset_index().groupby('customer_unique_id')\
.agg({'order_purchase_timestamp': lambda x: (present_day  - x.max()).days,
                                              'order_id': lambda x: len(x),
                                                'price': lambda x: x.sum()})
rfm_df.rename(columns={'order_purchase_timestamp': 'recency',
                          'order_id': 'frequency',
                          'price': 'monetary_value'}, inplace=True)
rfm_df.head()

rfm_df.describe().T

rfm_df.plot(
    kind='box',
    subplots=True,
    sharey=False,
    figsize=(10, 6)
)

# increase spacing between subplots
plt.subplots_adjust(wspace=0.5)
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(14, 7))

sns.histplot(data=rfm_df, x="recency", kde=True, color="skyblue", ax=axs[0])
sns.histplot(data=rfm_df, x="frequency", kde=True, color="olive", ax=axs[1])
sns.histplot(data=rfm_df, x="monetary_value", kde=True, color="gold", ax=axs[2])

plt.show()

# Calculate Z scores to normalize the data
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(rfm_df))
print(z)

rfm_clean = rfm_df[(z < 3).all(axis=1)]
rfm_clean.shape

fig, axs = plt.subplots(1, 3, figsize=(14, 7))

sns.histplot(data=rfm_clean, x="recency", kde=True, color="skyblue", ax=axs[0])
sns.histplot(data=rfm_clean, x="frequency", kde=True, color="olive", ax=axs[1])
sns.histplot(data=rfm_clean, x="monetary_value", kde=True, color="gold", ax=axs[2])

plt.show()

"""RFM Score"""

# Use quintiles to to make 5 equal parts based on the available values. Each quintiles contains 20% of the population.
quintiles = rfm_clean[['recency', 'frequency', 'monetary_value']].quantile([.2, .4, .6, .8]).to_dict()
quintiles

def r_score(x):
    if x <= quintiles['recency'][.2]:
        return 5
    elif x <= quintiles['recency'][.4]:
        return 4
    elif x <= quintiles['recency'][.6]:
        return 3
    elif x <= quintiles['recency'][.8]:
        return 2
    else:
        return 1

def fm_score(x, c):
    if x <= quintiles[c][.2]:
        return 1
    elif x <= quintiles[c][.4]:
        return 2
    elif x <= quintiles[c][.6]:
        return 3
    elif x <= quintiles[c][.8]:
        return 4
    else:
        return 5

#Calculate RFM score for each customer
 pd.options.mode.chained_assignment = None

rfm_clean['R'] = rfm_clean['recency'].apply(lambda x: r_score(x))
rfm_clean['F'] = rfm_clean['frequency'].apply(lambda x: fm_score(x, 'frequency'))
rfm_clean['M'] = rfm_clean['monetary_value'].apply(lambda x: fm_score(x, 'monetary_value'))
rfm_clean['RFM Score'] = rfm_clean['R'].map(str) + rfm_clean['F'].map(str) + rfm_clean['M'].map(str)
rfm_clean.head()

# Created 6 segments based on R and F scores

segments = {
    '[1-2][1-4]': 'at risk',
    '[1-2]5': 'can\'t loose',
    '3[1-3]': 'needs attention',
    '[3-4][4-5]': 'loyal customers',
    '[4-5]1': 'new customers',
    '[4-5][2-5]': 'champions'

}

rfm_clean['Segment'] = rfm_clean['R'].map(str) + rfm_clean['F'].map(str)
rfm_clean['Segment'] = rfm_clean['Segment'].replace(segments, regex=True)
rfm_clean.head()

# count the number of customers in each segment
segments_counts = rfm_clean['Segment'].value_counts().sort_values(ascending=True)

segments_counts

"""Цель RFM анализа сформировать сегменты и в зависимости от сегмента воздействовать на них определенным образом. Например: предложить бонус, льготу, отправить push или email уведомление. Важно делать это таргетировано.

Эффект от использования RFM анализа может быть следующим: удержание клиентов, повышение дохода, повышение лояльность клиентов.

Примеры интерпретации сегментов RFM анализа:

* R=5, F=5, M=5 — платят чаcто, много и недавно. Самые лояльным и активные пользователи.
* R=1, F=1, M=1 — платят мало, редко и давно. Скорее всего потерянные клиенты. Возможно не стоит предпринять действия по их возврату, если цена привлечения выше ожидаемой прибыли.
* R=1/2, F=4/5, M=4/5 — лояльные пользователи на грани ухода. Предлагаем им бонус, скидку и пытаемся их вернуть.
* R=4/5, F=1, M=1/2/3/4/5 — пользователи недавно совершили платеж. Пробуем их стимулировать покупать еще.
"""